{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ----- Hyperparameters ----- #\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "batch_size = 64\n",
    "block_size = 256\n",
    "train_val_split = 0.9\n",
    "embed_size = 192\n",
    "learning_rate = 3e-4\n",
    "head_count = 4\n",
    "layer_count = 4\n",
    "dropout = 0.2\n",
    "# --------------------------- #\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "with open('inputs.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "strings_to_int = {char: i for i, char in enumerate(chars)}\n",
    "int_to_strings = {i: char for i, char in enumerate(chars)}\n",
    "\n",
    "encode = lambda string: [strings_to_int[char] for char in string]\n",
    "decode = lambda tokens: ''.join([int_to_strings[token] for token in tokens])\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "split_index = int(train_val_split*len(data))\n",
    "train_data = data[:split_index]\n",
    "val_data = data[split_index:]\n",
    "\n",
    "def generate_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    indices = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    inputs = torch.stack([data[i:i+block_size] for i in indices])\n",
    "    targets = torch.stack([data[i+1:i+block_size+1] for i in indices])\n",
    "    \n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    \n",
    "    return inputs, targets\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" Single Head for Self-Attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.query = nn.Linear(embed_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(embed_size, head_size, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        batch_size, block_size, embed_size = inputs.shape # (B,T,C)\n",
    "        \n",
    "        queries = self.query(inputs) # what you want (B,T,C)\n",
    "        keys = self.key(inputs) # what you have (B,T,C)\n",
    "        values = self.value(inputs) # what fits best (B,T,C)\n",
    "        \n",
    "        weights = queries @ keys.transpose(-2, -1) * embed_size**-0.5 # sum of dot products of queries and keys for each token (B,T,T)\n",
    "        weights = weights.masked_fill(self.tril[:block_size, :block_size] == 0, float('-inf')) # mask out future tokens (B,T,T)\n",
    "        weights = F.softmax(weights, dim=-1) # normalize weights (B,T,T)\n",
    "        \n",
    "        weights = self.dropout(weights) # dropout\n",
    "        \n",
    "        output = weights @ values # weighted sum of values (B,T,C)\n",
    "        \n",
    "        return output # (B,T,C)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, head_count, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(head_count)])\n",
    "        self.projection = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        output = torch.cat([head(inputs) for head in self.heads], dim=-1)\n",
    "        output = self.projection(output)\n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_size, embed_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        return self.net(inputs)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, embed_size, head_count):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(head_count, embed_size//head_count)\n",
    "        self.feed_forward = FeedForward(embed_size)\n",
    "        self.layernorm1 = nn.LayerNorm(embed_size)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs + self.self_attention(self.layernorm1(inputs))\n",
    "        inputs = inputs + self.feed_forward(self.layernorm2(inputs))\n",
    "        return inputs\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embeddings = nn.Embedding(block_size, embed_size)\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[Block(embed_size, head_count) for _ in range(layer_count)])\n",
    "        self.layer_norm = nn.LayerNorm(embed_size)\n",
    "        \n",
    "        self.linear_output = nn.Linear(embed_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs, targets=None):\n",
    "        batch_size, block_size = inputs.shape\n",
    "        \n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_position = self.position_embeddings(torch.arange(block_size).to(device))\n",
    "        \n",
    "        x = embedded_tokens + embedded_position\n",
    "        x = self.blocks(x)\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.linear_output(x)\n",
    "        \n",
    "        if targets is not None:\n",
    "            batch, block_size, embed_size = logits.shape\n",
    "            logits = logits.view(batch*block_size, embed_size)\n",
    "            targets = targets.view(batch*block_size)\n",
    "            \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, inputs, max_tokens):\n",
    "        for _ in range(max_tokens):\n",
    "            inputs_cropped = inputs[:, -block_size:]\n",
    "            logits, loss = self(inputs_cropped)\n",
    "            logits = logits[:, -1, :]\n",
    "            probabilities = F.softmax(logits, dim=1)\n",
    "            next_index = torch.multinomial(probabilities, num_samples=1)\n",
    "            inputs = torch.cat([inputs, next_index], dim=1)\n",
    "        return inputs\n",
    "\n",
    "\n",
    "\n",
    "model = BigramModel()\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: 4.372765064239502\n",
      "Loss at step 100: 2.566945791244507\n",
      "Loss at step 200: 2.503253936767578\n",
      "Loss at step 300: 2.4448091983795166\n",
      "Loss at step 400: 2.427504539489746\n",
      "Loss at step 500: 2.408301591873169\n",
      "Loss at step 600: 2.3206517696380615\n",
      "Loss at step 700: 2.269308567047119\n",
      "Loss at step 800: 2.173673152923584\n",
      "Loss at step 900: 2.1101293563842773\n",
      "Loss at step 1000: 2.0719943046569824\n",
      "Loss at step 1100: 2.03831148147583\n",
      "Loss at step 1200: 1.9424840211868286\n",
      "Loss at step 1300: 1.8941841125488281\n",
      "Loss at step 1400: 1.855095386505127\n",
      "Loss at step 1500: 1.8359614610671997\n",
      "Loss at step 1600: 1.7790043354034424\n",
      "Loss at step 1700: 1.8070347309112549\n",
      "Loss at step 1800: 1.7414581775665283\n",
      "Loss at step 1900: 1.7269458770751953\n",
      "Loss at step 2000: 1.6682770252227783\n",
      "Loss at step 2100: 1.6946630477905273\n",
      "Loss at step 2200: 1.6537954807281494\n",
      "Loss at step 2300: 1.6401984691619873\n",
      "Loss at step 2400: 1.5940712690353394\n",
      "Loss at step 2500: 1.6288198232650757\n",
      "Loss at step 2600: 1.6138863563537598\n",
      "Loss at step 2700: 1.5974204540252686\n",
      "Loss at step 2800: 1.55300772190094\n",
      "Loss at step 2900: 1.5517116785049438\n",
      "Loss at step 3000: 1.56233549118042\n",
      "Loss at step 3100: 1.5517054796218872\n",
      "Loss at step 3200: 1.5224272012710571\n",
      "Loss at step 3300: 1.5548405647277832\n",
      "Loss at step 3400: 1.5352064371109009\n",
      "Loss at step 3500: 1.546668291091919\n",
      "Loss at step 3600: 1.502000331878662\n",
      "Loss at step 3700: 1.519871473312378\n",
      "Loss at step 3800: 1.4850233793258667\n",
      "Loss at step 3900: 1.4706742763519287\n",
      "Loss at step 4000: 1.5034040212631226\n",
      "Loss at step 4100: 1.4974747896194458\n",
      "Loss at step 4200: 1.429455041885376\n",
      "Loss at step 4300: 1.465118408203125\n",
      "Loss at step 4400: 1.4503737688064575\n",
      "Loss at step 4500: 1.446710228919983\n",
      "Loss at step 4600: 1.4620251655578613\n",
      "Loss at step 4700: 1.4081447124481201\n",
      "Loss at step 4800: 1.46806800365448\n",
      "Loss at step 4900: 1.4178831577301025\n"
     ]
    }
   ],
   "source": [
    "for steps in range(5000):\n",
    "    inputs, targets = generate_batch('train')\n",
    "    \n",
    "    logits, loss = model(inputs, targets)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if steps % 100 == 0:\n",
    "        print(f'Loss at step {steps}: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ULYCUS:\n",
      "Have sir, like othe two hard smogh.\n",
      "\n",
      "FLORIZEL:\n",
      "Speak, to but with countrares in sued evil.\n",
      "\n",
      "KING HENNRY VI:\n",
      "Mere it shoot not.\n",
      "\n",
      "ISABELLA:\n",
      "Epether, minied, what I am to, the subjectime's.\n",
      "Have you he scurd with friends to the wear, at,\n",
      "Upon make upon youth labour conciraties.\n",
      "\n",
      "ROMEO:\n",
      "Nothumberl' bange that meet, thou most hilst done\n",
      "Turn thy all be can'st: he carle sams\n",
      "Trought you, and accues a fear.\n",
      "\n",
      "POMPSINIUS:\n",
      "You know love would i' the noble betider denied.\n",
      "\n",
      "VANCIO:\n",
      "Fourn to deir the live by more of his whall\n",
      "\n",
      "ANGELO:\n",
      "Ay is flight, hence, it talking's to sequain'd.\n",
      "Now, hisde the darks and might for outher.\n",
      "\n",
      "PARIS:\n",
      "We was too sweet her ov to the prity.\n",
      "\n",
      "KING LEWICHARD's CLARD II:\n",
      "So, My doubty. Both, hast not state\n",
      "Vallino frown the of; as I well dauge must\n",
      "spent; and high noble about. Best what me I was the\n",
      "would against thee be pranemy yeard from palace wish\n",
      "Shapsion such, may not of you are a time\n",
      "Ken more is this sentent of thars, whilesy\n",
      "hast, you shall cauchame villai\n"
     ]
    }
   ],
   "source": [
    "input_val = torch.zeros(1, 1, dtype=torch.long).to(device)\n",
    "\n",
    "print(decode(model.generate(input_val, 1000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
